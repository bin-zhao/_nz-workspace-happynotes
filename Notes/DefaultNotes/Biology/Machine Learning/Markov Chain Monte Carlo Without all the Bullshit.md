
---

# Markov Chain Monte Carlo Without all the Bullshit

[#markov chain](https://www.jeremykun.com/tags/markov-chain/)Â Â [#mathematics](https://www.jeremykun.com/tags/mathematics/)Â Â [#MCMC](https://www.jeremykun.com/tags/mcmc/)Â Â [#monte carlo](https://www.jeremykun.com/tags/monte-carlo/)Â Â [#random walk](https://www.jeremykun.com/tags/random-walk/)Â 

2015-04-06

This article was ported from my old Wordpress blogÂ [here,](https://jeremykun.wordpress.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/)Â If you see any issues with the rendering or layout, pleaseÂ [send me an email](mailto:mathintersectprogramming@gmail.com).

I have a little secret: I donâ€™t like the terminology, notation, and style of writing in statistics. I find it unnecessarily complicated. This shows up when trying to read about Markov Chain Monte Carlo methods. Take, for example, the abstract to the Markov Chain Monte Carlo article in theÂ [Encyclopedia of Biostatistics.](https://doi.org/10.1002/0470011815.b2a14021)

> Markov chain Monte Carlo (MCMC) is a technique for estimating by simulation the expectation of a statistic in a complex model. Successive random selections form a Markov chain, the stationary distribution of which is the target distribution. It is particularly useful for the evaluation of posterior distributions in complex Bayesian models. In the Metropolisâ€“Hastings algorithm, items are selected from an arbitrary â€œproposalâ€ distribution and are retained or not according to an acceptance rule. The Gibbs sampler is a special case in which the proposal distributions are conditional distributions of single components of a vector parameter. Various special cases and applications are considered.

I can only vaguely understand what the author is saying here (and really only because I know ahead of time what MCMC is). There are certainly references to more advanced things than what Iâ€™m going to cover in this post. But it seems very difficult to find an explanation of Markov Chain Monte CarloÂ _without_Â superfluous jargon. The â€œbullshitâ€ here is the implicit claim of an author that such jargon is needed. Maybe it is to explain advanced applications (like attempts to do â€œinference in Bayesian networksâ€), but it is certainly not needed to define or analyze the basic ideas.

So to counter, hereâ€™s my own explanation of Markov Chain Monte Carlo, inspired by the treatment ofÂ [John Hopcroft and Ravi Kannan](http://web.archive.org/web/20150204160331/http://research.microsoft.com/en-US/people/kannan/book-no-solutions-aug-21-2014.pdf).

## The Problem is Drawing from a Distribution

Markov Chain Monte Carlo is a technique to solve the problem ofÂ _sampling from a complicated distribution._Â Let me explain by the following imaginary scenario. Say I have a magic box which can estimate probabilities of baby names very well. I can give it a string like â€œMalcolmâ€ and it will tell me the exact probabilityÂ pMalcolmÂ that you will choose this name for your next child. So thereâ€™s a distributionÂ DÂ over all names, itâ€™s very specific to your preferences, and for the sake of argument say this distribution is fixed and you donâ€™t get to tamper with it.

Now comes the problem: I want toÂ _efficiently draw_Â a name from this distributionÂ D. This is the problem that Markov Chain Monte Carlo aims to solve. Why is it a problem? Because I have no idea what process you use to pick a name, so I canâ€™t simulate that process myself. Hereâ€™s another method you could try: generate a nameÂ xÂ uniformly at random, ask the machine forÂ px, and then flip a biased coin with probabilityÂ pxÂ and useÂ xÂ if the coin lands heads. The problem with this is that there are exponentially many names! The variable here is the number of bits needed to write down a nameÂ n=|x|. So either the probabilitiesÂ pxÂ will be exponentially small and Iâ€™ll be flipping for a very long time to get a single name, or else there will only be a few names with nonzero probability and it will take me exponentially many draws to find them. Inefficiency is the death of me.

So this is a serious problem! Letâ€™s restate it formally just to be clear.

**Definition (The sampling problem):**Â LetÂ DÂ be a distribution over a finite setÂ X. You are given black-box access to the probability distribution functionÂ p(x)Â which outputs the probability of drawingÂ xâˆˆXÂ according toÂ D. Design an efficient randomized algorithmÂ AÂ which outputs an element ofÂ XÂ so that the probability of outputtingÂ xÂ is approximatelyÂ p(x). More generally, output a sample of elements fromÂ XÂ drawn according toÂ p(x).

Assume thatÂ AÂ has access to only fair random coins, though this allows one to efficiently simulate flipping aÂ [biased coin of any desired probability](https://www.jeremykun.com/2014/02/12/simulating-a-biased-coin-with-a-fair-coin/ "Simulating a Biased Coin with a Fair Coin").

Notice that with such an algorithm weâ€™d be able to do things like estimate the expected value of some random variableÂ f:Xâ†’R. We could take a large sampleÂ SâŠ‚XÂ via the solution to the sampling problem, and then compute the average value ofÂ fÂ on that sample. This is what a Monte Carlo method does when sampling is easy. In fact, the Markov Chain solution to the sampling problem will allow us to do the samplingÂ _and_Â the estimation ofÂ E(f)Â in one fell swoop if you want.

But the core problem is really a sampling problem, and â€œMarkov Chain Monte Carloâ€ would be more accurately called the â€œMarkov Chain Sampling Method.â€ So letâ€™s see why a Markov Chain could possibly help us.

## Random Walks, the â€œMarkov Chainâ€ part of MCMC

Markov Chain is essentially a fancy term for a random walk on a graph.

You give me a directed graphÂ G=(V,E), and for each edgeÂ e=(u,v)âˆˆEÂ you give me a numberÂ pu,vâˆˆ[0,1]. In order to make a random walk make sense, theÂ pu,vÂ need to satisfy the following constraint:

For any vertexÂ xâˆˆV, the set all valuesÂ px,yÂ on outgoing edgesÂ (x,y)Â must sum to 1, i.e. form a probability distribution.

If this is satisfied then we can take a random walk onÂ GÂ according to the probabilities as follows: start at some vertexÂ x0. Then pick an outgoing edge at random according to the probabilities on the outgoing edges, and follow it toÂ x1. Repeat if possible.

I say â€œif possibleâ€ because an arbitrary graph will not necessarily have any outgoing edges from a given vertex. Weâ€™ll need to impose some additional conditions on the graph in order to apply random walks to Markov Chain Monte Carlo, but in any case the idea of randomly walking is well-defined, and we call the whole objectÂ (V,E,{pe}eâˆˆE)Â aÂ _Markov chain._

Here is an example where the vertices in the graph correspond to emotional states.

![[6f48b6bdec998f61642b93fa56ff469b_MD5.gif]]

#### An example Markov chain

In statistics land, they take the â€œstateâ€ interpretation of a random walk very seriously. They call the edge probabilities â€œstate-to-state transitions.â€ The main theorem we need to do anything useful with Markov chains is the stationary distribution theorem (sometimes called the â€œFundamental Theorem of Markov Chains,â€ and for good reason). What it says intuitively is that for a very long random walk, the probability that you end at some vertexÂ vÂ is independent of where you started! All of these probabilities taken together is called theÂ _stationary distribution_Â of the random walk, and it is uniquely determined by the Markov chain.

However, for the reasons we stated above (â€œif possibleâ€), the stationary distribution theorem is not true of every Markov chain. The main property we need is that the graphÂ GÂ isÂ _strongly connected._Â Recall that a directed graph is called connected if, when you ignore direction, there is a path from every vertex to every other vertex. It is calledÂ _strongly connected_Â if you still get paths everywhere when considering direction. If we additionally require the stupid edge-case-catcher that no edge can have zero probability, then strong connectivity (of one component of a graph) is equivalent to the following property:

For every vertexÂ vâˆˆV(G), an infinite random walk started atÂ vÂ will return toÂ vÂ with probability 1.

In fact it will return infinitely often. This property is called theÂ _persistence_Â of the stateÂ vÂ by statisticians. I dislike this term because it appears to describe a property of a vertex, when to me it describes a property of the connected component containing that vertex. In any case, since in Markov Chain Monte Carlo weâ€™ll be picking the graph to walk on (spoiler!) we will ensure the graph is strongly connected by design.

Finally, in order to describe the stationary distribution in a more familiar manner (using linear algebra), we will write the transition probabilities as a matrixÂ AÂ where entryÂ aj,i=p(i,j)Â if there is an edgeÂ (i,j)âˆˆEÂ and zero otherwise. Here the rows and columns correspond to vertices ofÂ G, and eachÂ _column_Â iÂ forms the probability distribution of going from stateÂ iÂ to some other state in one step of the random walk. NoteÂ AÂ is the transpose of the weighted adjacency matrix of the directed weighted graphÂ GÂ where the weights are the transition probabilities (the reason I do it this way is because matrix-vector multiplication will have the matrix on the left instead of the right; see below).

This matrix allows me to describe things nicely using the language of linear algebra. In particular if you give me a basis vectorÂ eiÂ interpreted as â€œthe random walk currently at vertexÂ i,â€ thenÂ AeiÂ gives a vector whoseÂ j-th coordinate is the probability that the random walk would be at vertexÂ jÂ after one more step in the random walk. Likewise, if you give me a probability distributionÂ qÂ over the vertices, thenÂ AqÂ gives a probability vector interpreted as follows:

If a random walk is in stateÂ iÂ with probabilityÂ qi, then theÂ j-th entry ofÂ AqÂ is the probability that after one more step in the random walk you get to vertexÂ j.

Interpreted this way, the stationary distribution is a probability distributionÂ Ï€Â such thatÂ AÏ€=Ï€, in other wordsÂ Ï€Â is an eigenvector ofÂ AÂ with eigenvalue 1.

A quick side note for avid readers of this blog: this analysis of a random walk is exactly what we did back in the early days of this blog when we studiedÂ [the PageRank algorithm](https://www.jeremykun.com/2011/06/12/googles-pagerank-introduction/ "Googleâ€™s PageRank â€“ Introduction")Â for ranking webpages. There we called the matrixÂ AÂ â€œa web matrix,â€ did random walks on it, and found a special eigenvalue whose eigenvector was a â€œstationary distributionâ€ that we used to rank web pages (this used something called theÂ [Perron-Frobenius theorem](https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem), which says a random-walk matrix has that special eigenvector). There we described an algorithm to actually find that eigenvector by iteratively multiplyingÂ A. The following theorem is essentially a variant of this algorithm but works under weaker conditions; for the web matrix we added additional â€œfakeâ€ edges that give the needed stronger conditions.

**Theorem:**Â LetÂ GÂ be a strongly connected graph with associated edge probabilitiesÂ {pe}eâˆˆEÂ forming a Markov chain. For a probability vectorÂ x0, defineÂ xt+1=AxtÂ for allÂ tâ‰¥1, and letÂ vtÂ be the long-term averageÂ vt=1tâˆ‘s=1txs. Then:

1. There is a unique probability vectorÂ Ï€Â withÂ AÏ€=Ï€.
2. For allÂ x0, the limitÂ limtâ†’âˆvt=Ï€.

_Proof._Â SinceÂ vtÂ is a probability vector we just want to show thatÂ |Avtâ€“vt|â†’0Â asÂ tâ†’âˆ. Indeed, we can expand this quantity as

Avtâ€“vt=1t(Ax0+Ax1+â‹¯+Axtâˆ’1)â€“1t(x0+â‹¯+xtâˆ’1)Â =1t(xtâ€“x0)

ButÂ xt,x0Â are unit vectors, so their difference is at most 2, meaningÂ |Avtâ€“vt|â‰¤2tâ†’0. Now itâ€™s clear that this does not depend onÂ v0. For uniqueness we will cop out and appeal to the Perron-Frobenius theorem that says any matrix of this form has a unique such (normalized) eigenvector.

â—»

One additional remark is that, in addition to computing the stationary distribution by actually computing this average or using an eigensolver, one can analytically solve for it as the inverse of a particular matrix. DefineÂ B=Aâˆ’In, whereÂ InÂ is theÂ nÃ—nÂ identity matrix. LetÂ CÂ beÂ BÂ with a row of ones appended to the bottom and the topmost row removed. Then one can show (quite opaquely) that the last column ofÂ Câˆ’1Â isÂ Ï€. We leave this as an exercise to the reader, because Iâ€™m pretty sure nobody uses this method in practice.

One final remark is about why we need to take an average over all ourÂ xtÂ in the theorem above. There is an extra technical condition one can add to strong connectivity, calledÂ _aperiodicity_, which allows one to beef up the theorem so thatÂ xtÂ itself converges to the stationary distribution. Rigorously, aperiodicity is the property that, regardless of where you start your random walk, after some sufficiently large number of stepsÂ nÂ the random walk has a positive probability of being at every vertex at every subsequent step. As an example of a graph where aperiodicity fails: an undirected cycle on an even number of vertices. In that case there will only be a positive probability of being at certain vertices everyÂ _other_Â step, and averaging those two long term sequences gives the actual stationary distribution.

![[7cb071bacaf61456acb8e3813bce7732_MD5.png]]

#### Image source: Wikipedia

One way to guarantee that your Markov chain is aperiodic is to ensure there is a positive probability of staying at any vertex. I.e., that your graph has a self-loop. This is what weâ€™ll do in the next section.

## Constructing a graph to walk on

Recall that the problem weâ€™re trying to solve is to draw from a distribution over a finite setÂ XÂ with probability functionÂ p(x). The MCMC method is to construct a Markov chain whose stationary distribution is exactlyÂ p, even when you just have black-box access to evaluatingÂ p. That is, you (implicitly) pick a graphÂ GÂ and (implicitly) choose transition probabilities for the edges to make the stationary distributionÂ p. Then you take a long enough random walk onÂ GÂ and output theÂ xÂ corresponding to whatever state you land on.

The easy part is coming up with a graph that has the right stationary distribution (in fact, â€œmostâ€ graphs will work). The hard part is to come up with a graph where you can prove that the convergence of a random walk to the stationary distribution is fast in comparison to the size ofÂ X. Such a proof is beyond the scope of this post, but the â€œrightâ€ choice of a graph is not hard to understand.

The one weâ€™ll pick for this post is called theÂ **Metropolis-Hastings**Â algorithm. The input is your black-box access toÂ p(x), and the output is a set of rules that implicitly define a random walk on a graph whose vertex set isÂ X.

It works as follows: you pick some way to putÂ XÂ on a lattice, so that each state corresponds to some vector inÂ {0,1,â€¦,n}d. Then you add (two-way directed) edges to all neighboring lattice points. ForÂ n=5,d=2Â it would look like this:

[![[39cb2c5bda7150a102d403498763ede3_MD5.jpg]]](https://www.jeremykun.com/img/2015/2dlattice.jpg)

#### Image credit http://www.ams.org/samplings/feature-column/fcarc-taxi

And forÂ d=3,nâˆˆ{2,3}Â it would look like this:

[![[1834cfd29049fe0f2683cb643b19700c_MD5.gif]]](https://www.jeremykun.com/img/2015/lattice.gif)

#### lattice

You have to be careful here to ensure the vertices you choose forÂ XÂ are not disconnected, but in many applicationsÂ XÂ is naturally already a lattice.

Now we have to describe the transition probabilities. LetÂ rÂ be the maximum degree of a vertex in this lattice (r=2d). Suppose weâ€™re at vertexÂ iÂ and we want to know where to go next. We do the following:

1. Pick neighborÂ jÂ with probabilityÂ 1/rÂ (there is some chance to stay atÂ i).
2. If you picked neighborÂ jÂ andÂ p(j)â‰¥p(i)Â then deterministically go toÂ j.
3. Otherwise,Â p(j)<p(i), and you go toÂ jÂ with probabilityÂ p(j)/p(i).

We can state the probability weightÂ pi,jÂ on edgeÂ (i,j)Â more compactly as

pi,j=1rmin(1,p(j)/p(i))Â pi,i=1â€“âˆ‘(i,j)âˆˆE(G);jâ‰ ipi,j

It is easy to check that this is indeed a probability distribution for each vertexÂ i. So we just have to show thatÂ p(x)Â is the stationary distribution for this random walk.

Hereâ€™s a fact to do that: if a probability distributionÂ vÂ with entriesÂ v(x)Â for eachÂ xâˆˆXÂ has the property thatÂ v(x)px,y=v(y)py,xÂ for allÂ x,yâˆˆX, theÂ vÂ is the stationary distribution. To prove it, fixÂ xÂ and take the sum of both sides of that equation over allÂ y. The result is exactly the equationÂ v(x)=âˆ‘yv(y)py,x, which is the same asÂ v=Av. Since the stationary distribution is the unique vector satisfying this equation,Â vÂ has to be it.

Doing this with out chosenÂ p(i)Â is easy, sinceÂ p(i)pi,jÂ andÂ p(i)pj,iÂ are both equal toÂ 1rmin(p(i),p(j))Â by applying a tiny bit of algebra to the definition. So weâ€™re done! One can just randomly walk according to these probabilities and get a sample.

## Last words

The last thing I want to say about MCMC is to show that you can estimate the expected value of a functionÂ E(f)Â simultaneously while random-walking through your Metropolis-Hastings graph (or any graph whose stationary distribution isÂ p(x)). By definition the expected value ofÂ fÂ isÂ âˆ‘xf(x)p(x).

Now what we can do is compute the average value ofÂ f(x)Â just among those states weâ€™ve visited during our random walk. With a little bit of extra work you can show that this quantity will converge to the true expected value ofÂ fÂ at about the same time that the random walk converges to the stationary distribution. (Here the â€œaboutâ€ means weâ€™re off by a constant factor depending onÂ f). In order to prove this you need some extra tools Iâ€™m too lazy to write about in this post, but the point is that it works.

The reason I did not start by describing MCMC in terms of estimating the expected value of a function is because the core problem is a sampling problem. Moreover, there are many applications of MCMC that need nothing more than a sample. For example, MCMC can be used to estimate the volume of an arbitrary (maybe high dimensional) convex set. SeeÂ [these lecture notes](http://www.cs.berkeley.edu/~sinclair/cs294/n1.pdf)Â of Alistair Sinclair for more.

If demand is popular enough, I could implement the Metropolis-Hastings algorithm in code (it wouldnâ€™t be industry-strength, but perhaps illuminating? Iâ€™m not so sureâ€¦).

Until next time!

---

Want to respond?Â [Send me an email](mailto:mathintersectprogramming@gmail.com),Â [post a webmention](https://webmention.io/www.jeremykun.com/webmention), or find meÂ [elsewhere on the internet](https://www.jeremykun.com/about/).

DOI:Â [https://doi.org/10.59350/7b0tq-90902](https://doi.org/10.59350/7b0tq-90902)

---

## Webmentions

- Â [![[030a42528f758783663a61def28a8861_MD5.webp]]Â ğŸ’¬](https://lemmy.bestiver.se/post/329069 "lemmy.bestiver.se mentioned")Â [lemmy.bestiver.se](https://lemmy.bestiver.se/post/329069)Â (mention)
- Â [![[122df338b1d29873b5a72fb80ecedf5c_MD5.jpg]]Â â¤ï¸](https://mathstodon.xyz/@j2kun/114348646006516100#favorited-by-109588097832763669 "Dan Farmer liked")Â [Dan Farmer](https://mathstodon.xyz/@j2kun/114348646006516100#favorited-by-109588097832763669)Â at mathstodon.xyz
- Â [![[8562583dfb509250f02f671591ff77be_MD5.jpg]]Â â¤ï¸](https://mathstodon.xyz/@j2kun/114348646006516100#favorited-by-109309182983458683 "Graham Coleman liked")Â [Graham Coleman](https://mathstodon.xyz/@j2kun/114348646006516100#favorited-by-109309182983458683)Â at mathstodon.xyz
- Â [![[2911485834bab96be79f00486cfdbe2c_MD5.png]]Â â¤ï¸](https://mathstodon.xyz/@j2kun/114348646006516100#favorited-by-111310104592373990 "Rich Loveland liked")Â [Rich Loveland](https://mathstodon.xyz/@j2kun/114348646006516100#favorited-by-111310104592373990)Â at mathstodon.xyz
- Â [![[030a42528f758783663a61def28a8861_MD5.webp]]Â â­ï¸](https://news.ycombinator.com/item?id=9331808 "diego898 bookmarked")Â [diego898](https://news.ycombinator.com/item?id=9331808)Â via Hacker News (2015-04-07)
- Â [![[030a42528f758783663a61def28a8861_MD5.webp]]Â â­ï¸](https://news.ycombinator.com/item?id=12537043 "sebg bookmarked")Â [sebg](https://news.ycombinator.com/item?id=12537043)Â via Hacker News (2016-09-20)
- Â [![[030a42528f758783663a61def28a8861_MD5.webp]]Â â­ï¸](https://news.ycombinator.com/item?id=33332437 "larve bookmarked")Â [larve](https://news.ycombinator.com/item?id=33332437)Â via Hacker News (2022-10-25)
- Â [![[030a42528f758783663a61def28a8861_MD5.webp]]Â â­ï¸](https://news.ycombinator.com/item?id=43700633 "ibobev bookmarked")Â [ibobev](https://news.ycombinator.com/item?id=43700633)Â via Hacker News (2025-04-16)

---

